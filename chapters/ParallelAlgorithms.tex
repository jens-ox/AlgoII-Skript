\chapter{Parallele Algorithmen}

\section{Einleitung}

Indem man Parallelverarbeitung verwendet, kann man sowohl Ressourcen einsparen als auch Ressourcenrestriktionen brechen:
\begin{itemize}
  \item \textbf{Zeitersparnis}: Arbeiten \( p \) Computer an einem Problem, so sind sie bis zu \( p \) mal so schnell.
  \item \textbf{Kommunikationsersparnis}: Fallen Daten verteilt an, so kann man sie auch verteilt (vor)verarbeiten.
  \item \textbf{Energieersparnis}: Zwei Prozessoren mit halber Taktfrequenz brauchen weniger Energie als ein voll getakteter Prozessor.
  \item \textbf{Speicherbeschränkung}: Mehr Prozessoren haben mehr Hauptspeicher, mehr Cache,\dots
\end{itemize}

Es gibt sehr viele Modelle der Parallelverarbeitung, wir werden hier allerdings nur zwei Standardmodelle diskutieren:
\begin{itemize}
  \item \textbf{Prozessornetzwerke mit} \term{Nachrichtenkopplung}\index{Nachrichtenkopplung}.

    Prozessoren sind hier ``normale CPUs'' mit lokalen Speicher. Gemeinsam mit seinem lokalen Speicher wird eine lokale CPU auch \emph{processing element} (PE) genannt. Der Datenaustausch passiert über ein Netzwerk in Form von Nachrichten zwischen zwei Prozessoren.
  \item \textbf{Parallele Registermaschinen mit} \term{Speicherkopplung}\index{Speicherkopplung}.

    Auch hier wird mit normalen CPUs gearbeitet, allerdings haben diese keinen lokalen Speicher, sondern sind über ein Netzwerk mit Speichermodulen verbunden, auf welche alle CPUs zugreifen können. Der Datenaustausch erfolgt über das Netzwerk zwischen einer CPU und einem Speicher.
\end{itemize}

Auf nachrichtengekoppelte Rechner werden wir genauer eingehen.

\begin{figure}[H]
  \includegraphics[width=0.9\textwidth]{ParallelArchitectures}
  \caption{Vergleich zwischen \emph{parallelen Registermaschinen mit Speicherkopplung} (links) und einem \emph{Prozessornetzwerk mit Nachrichtenkopplung} (rechts)}
\end{figure}

\subsection{Nachrichtenkopplung vs. Speicherkopplung}

Neben den oben erläuterten Vorteilen von Parallelverarbeitung haben beide Modelle auch Nachteile:

\begin{itemize}
  \item \textbf{Nachrichtenkopplung}:
  \begin{itemize}
    \item Zwei Prozessoren werden zum Datentransport benötigt. Das passt dem Empfänger nicht immer.
    \item Parallelismus muss explizit programmiert werden.
  \end{itemize}
  \item \textbf{Speicherkopplung}:
  \begin{itemize}
    \item \emph{Skalierbarkeit}: Ist eine große Anzahl an Prozessoren sinnvoll?
    \item \emph{Kostenmaß} bei Speicherzugriffskonflikten?
  \end{itemize}
\end{itemize}

Als eine gute Strategie hat es sich erwiesen, den Entwurf für einen verteilten Speicher durchzuführen, da dieser einen viel breiteren Bereich abdecken kann. Die Implementierung erfolgt dann gegebenenfalls für einen gemeinsamen Speicher.

\section{Nachrichtengekoppelte Parallelrechner}

\subsection{Modell}

\begin{itemize}
  \item \textbf{Netzwerk}: Vollständig verknüpftes Punkt-zu-Punkt-Netzwerk
  \begin{itemize}
    \item voll-duplex
    \item Nachrichten überholen sich nicht
  \end{itemize}
  \item \textbf{Prozessoren}: können jeweils maximal gleichzeitig
  \begin{itemize}
    \item eine Nachricht an einen beliebigen Empfänger senden (send(smsg,to))
    \item eine Nachricht von einem beliebigen Sender empfangen (rmsg \( \coloneqq \) recv(from))
    \item \emph{oder} beides gleichzeitig (rmsg \( \coloneqq \) sendRecv(smsg, to, from))
  \end{itemize}
\end{itemize}

Als \emph{Kostenmodell} für das Senden oder Empfangen von \( l \) Bytes verwenden wir
\begin{equation*}
  T_\text{comm}(l) = T_\text{start} + l*T_\text{byte}\text{,}
\end{equation*}
wobei in der Praxis meist \( T_\text{byte} \ll T_\text{start} \). Ignoriert wird hier unter anderem der ``Abstand'' zwischen Sender und Empfänger.

Als \emph{Programmiermodell} verwenden wir \term{SPMD}\index{SPMD} (\emph{single program multiple data}). Alle PEs führen hier dasselbe Programm aus, unterschieden wird lediglich durch ``Ränge'' der PEs (paarweise verschiedene PE-Nummern).

\subsection{Parallele Reduktion}

Im Folgenden gehen wir über die grundlegenden Werkzeuge, die wir benötigen, um parallele Programme analysieren zu können.

\begin{definition}[Reduktion]
  Sei \( \otimes \) eine binäre, assoziative Operation auf einer Menge \( M \).

  Für \( x = (x_0,\dots,x_{p-1}) \in M \) definieren wir
  \begin{equation*}
    R_\otimes(x) = \bigotimes_{i < p}x_i = x_0 \otimes \cdots \otimes x_{p-1}\text{.}
  \end{equation*}
\end{definition}

Nun gilt folgender Satz:

\begin{theorem}
  Wenn \( \otimes \) eine binäre, assoziative Operation ist und \( p \) Elemente \( x_0,\dots,x_{p-1} \) auf \( p \) PEs verteilt sind, dann kann man \( \bigotimes_{i < p}x_i \) in Zeit \( O(\log p) \) auf PE \( 0 \) berechnen.
\end{theorem}

\begin{figure}[H]
  \includegraphics[width=0.6\textwidth]{parallelExecution}
  \caption{Idee hinter obigem Satz}
\end{figure}

Sequenziell wäre die optimale Laufzeit \( O(n) \) gewesen. Auf \( p = n \) PEs hätte man nach obigem Satz eine Laufzeit von \( O(\log n) \) erreicht, also eine Beschleunigung von \( O\left( \frac{n}{\log n} \right) \). ``Ideal'' wäre eine Beschleunigung von \( O(p) = O(n) \) gewesen.

Wie kann man die Beschleunigung noch weiter verbessern?

\subsection{Parallele Reduktion mit \( p < n \)}

Verwenden wir nun \( p < n \) viele PEs, um eine Reduktion auf \( n \) Elementen durchzuführen, so erhält jedes PE \( n/p \) Datenelemente. Die PEs berechnen zuerst die Reduktion der lokalen Elemente, und anschließend wird auf diesen Reduktionen eine parallele Reduktion durchgeführt.
Laufzeit hierfür ist \( O(n/p) + O(\log p) \) und die Beschleunigung somit
\begin{equation*}
  \frac{O(n)}{O(n/p + \log p)}\text{.}
\end{equation*}
Ist \( p \in O(n/\log n) \), so ist die Beschleunigung \( O(p) \).

Man kann also durch Verringerung der Prozessorzahl \( p \) die Beschleunigung in die Nähe von \( p \) bringen. Dieses Prinzip nennt man \term{Brent's Prinzip}\index{Brent's Prinzip}.

\subsection{Analyse paralleler Programme}

Wir interessieren uns in erster Linie für
\begin{itemize}
  \item Laufzeit,
  \item Beschleunigung und
  \item verrichtete Arbeit.
\end{itemize}

Es ist
\begin{itemize}
  \item \( T_\text{par}(I,p) \) die parallele Laufzeit der Probleminstanz \( I \), bearbeitet mit \( p \) PEs,
  \item \( T_\text{seq}(I) \) die sequenzielle Laufzeit der Probleminstanz \( I \) mit dem ``besten bekannten Algorithmus''.
\end{itemize}

Im Allgemeinen ist \( T_\text{seq}(I) < T_\text{par}(I,1) \). Man erhält aber leicht Gleichheit, indem man den parallelen Algorithmus einfach den sequenziellen ausführen lässt, falls nur ein PE zur Verfügung steht.

\begin{definition}[Speedup]
  Wir definieren den \term{Speedup}\index{Speedup} als
  \begin{equation*}
    S(I,p) = \frac{T_\text{seq}(I)}{T_\text{par}(I,p)}\text{.}
  \end{equation*}
  Vergröbert für alle Probleminstanzen der Größe \( n \):
  \begin{equation*}
    S(n,p) = \inf\left \{ S(I,p) : n = \left\vert I \right\vert \right \}\text{.}
  \end{equation*}
\end{definition}

Zur Berechnung des Speedups können wir praktischerweise folgende Spezialfälle benutzen, die für Instanzen \( I \), \( I' \) gleicher Größe \( n \) gelten:
\begin{itemize}
  \item \( T_\text{par}(I,p) = T_\text{par}(I',p) = T_\text{par}(n,p) \),
  \item \( T_\text{seq}(I) = T_\text{seq}(I') = T_\text{seq}(n) \)
\end{itemize}

Simuliert man \( p \) Prozessoren durch einen Prozessor, so sehen wir, dass ein sequenzieller Algorithmus nie langsamer als \( O(p*T_\text{par}(I,p)) \) sein kann, weswegen immer \( \frac{T_\text{seq}(I)}{T_\text{par}(I,p)} \in O(p) \) ist und \( S(n,p) \in O(p) \) ebenfalls.

\begin{definition}[Effizienz]
  Wir definieren die \term{Effizienz}\index{Effizienz} eines parallelen Algorithmus als
  \begin{equation*}
    E(n,p) \coloneqq \frac{S(n,p)}{p}\text{.}
  \end{equation*}
  Es ist \( S(n,p) \in O(p) \) und daher \( E(n,p) \in O(1) \). Tatsächlich ist sogar \( E(n,p) > 1 \) möglich (sogenannter \emph{superlinearer Speedup}).
\end{definition}

Es ist
\begin{equation*}
  E(n,p) \in \frac{T_\text{par}(n,p)}{p*T_\text{seq}(n)}\text{.}
\end{equation*}

\begin{definition}[Arbeit]
  Wir definieren \term{Arbeit}\index{Arbeit} als
  \begin{equation*}
    W(n,p) \coloneqq p*T_\text{par}(n,p) \text{.}
  \end{equation*}
\end{definition}

Mit obiger Definition gilt
\begin{equation*}
  E(n,p) = \frac{S(n,p)}{p} = \frac{T_\text{seq}(n)}{T_\text{par}(n,p)*p} = \frac{T_\text{seq}(n)}{W(n,p)}\text{.}
\end{equation*}

Wir können nun die parallele Reduktion mit \( p < n \) von vorhin genauer analysieren:
\begin{itemize}
  \item Es werden zuerst \( \tfrac{n}{p} \) Elemente sequenziell und anschließend \( p \) partielle Summen reduziert.
  \item \textbf{Laufzeit}: \( T_\text{par}(n,p) = O(n/p) + O(\log p) \)
  \item \textbf{Beschleunigung}: \( S(n,p) = \frac{O(n)}{O(n/p + \log p)} \)
  \item \textbf{Effizienz}: \( E(n,p) = \frac{O(n)}{O(n+p\log p)} \)
\end{itemize}

Wir betrachten noch die zwei Sonderfälle:
\begin{itemize}
  \item \( p = n \):
  \begin{itemize}
    \item \( S(n,n) \in O\left( \frac{n}{\log n} \right) \)
    \item \( E(n,n) \in O\left( \frac{1}{\log n} \right) \)
  \end{itemize}
  \item \( p \in O\left( \frac{n}{\log n} \right) \):
  \begin{itemize}
    \item \( S(n,p) = O(p) \)
    \item \( E(n,p) = O(1) \)
  \end{itemize}
\end{itemize}

Wir erkennen hier die größere Effizienz für kleinere \( p \) (nach Brents Prinzip).

\subsection{Parallele Präfixsummen}

Wir verwenden \( \otimes \) wie oben definiert. Wir definieren nun
\begin{align*}
  &P_\otimes(x) = y = (y_0,\dots,y_{p-1}) \\
  \text{mit } &y_i = \bigotimes_{k \leq i}x_k\text{.} %\text{ oder } y_i = \bigotimes_{k < i}x_k \text{ falls neutrales Element \( e \).}
\end{align*}
Es ist also \( y_0 = x_0 \) und \( y_{i+1} = y_i \otimes x_{i+1} \).

\subsection{Präfixsummen --- Hyperwürfel-Algorithmus}

Wir machen es uns hier einfach und legen fest, dass \( p = n = 2^d \) (für \( d \in \N \)) und \( \otimes \) kommutativ.

Jede PE erhält nun eine ``Koordinate'' \( 0 \leq i \leq 2^d-1 \). Diese wird als Bitvektor
\begin{equation*}
  i = (i_{d-1}\cdots i_0) \quad \text{mit} \quad i_j \in \left \{ 0,1 \right \}
\end{equation*}
repräsentiert. Hier ist \( i_k \) das \( k \)-te Bit von rechts in \( i \).

Wir erlauben Kommunikation zwischen zwei PE \( i \) und \( i' \) nur, wenn die Hammingdistanz ihrer Bitvektoren \( 1 \) ist, sie sich also nur in einer Stelle unterscheiden. Wir erhalten so einen \emph{Hyperwürfel} aus PEs.

Wir berechnen nun Präfixsummen auf einem solchen Hyperwürfel:

\begin{pseudocode}
  \textbf{\textsc{prefixSum}}\( (x,\otimes) \) \\
  \( y \coloneqq x \) \enskip \textcolor{gray}{// auf PE \( i \) liegt \( x_i \)} \\
  \( s \coloneqq x \) \enskip \textcolor{gray}{// für Summe von Elementen in Unterwürfel} \\
  \textbf{for} \( k \coloneqq 0 \) \textbf{to} \( d - 1 \) \textbf{do} \\
  \phantom{\enskip} \( s' \coloneqq \textsc{sendRecv}(s,i \oplus 2^k, i \oplus 2^k) \) \\
  \phantom{\enskip} \( s \coloneqq s \otimes s' \) \\
  \phantom{\enskip} \textbf{if} \( i_k \equiv 1 \) \textbf{then} \( y \coloneqq y \otimes s' \) \\
  \textcolor{gray}{// auf PE \( i \) liegt \( y_i = x_0 \otimes \cdots \otimes x_i \)}
\end{pseudocode}

Wir erhalten eine Laufzeit
\begin{equation*}
  T_\text{prefix} \in O((T_\text{start} + l * T_\text{byte}) * \log p)\text{.}
\end{equation*}

Diese Laufzeit ist nicht optimal. Wie man sie optimieren kann wird in der Vorlesung ``Parallele Algorithmen'' näher erläutert.

\subsection{Paralleles Sortieren}

Hier gibt es zwei verschiedene Aufgabenvarianten:

\begin{enumerate}
  \item Alle \( n \) Elemente liegen zu Beginn auf PE 0. Deswegen muss jedes Element von PE 0 mindestens einmal angefasst werden. Die Laufzeit ist daher in \( \Omega(n) \).
  \item Je \( n/p \) Elemente liegen zu Beginn auf PE \( i \). Dieser Fall ist wesentlich interessanter.
\end{enumerate}

Zunächst betrachten wir den einfachen Fall \( p = n \) (Prozessor \( i \) hat Eingabeelement \( x_i \)). Wir behalten die Grundidee von Quicksort bei:
\begin{itemize}
  \item Wir wählen ein Element pv als Pivot.
  \item Elemente werden umverteilt:
  \begin{itemize}
    \item kleiner als pv: auf Prozessoren mit kleineren Rängen
    \item größer als pv: auf Prozessoren mit großen Rängen
  \end{itemize}
  \item Parallele Rekursion.
\end{itemize}

Wir schauen uns den \term{Theoretiker-Quicksort}\index{Theoretiker-Quicksort} an:

\begin{pseudocode}
  \textcolor{gray}{// Teil 0: Vorbereitungen} \\
  \textbf{\textsc{theoQSort0}}\( (x) \) \\
  \( i \coloneqq \text{commRank}() \) \\
  \( p \coloneqq \text{commSize}() \) \enskip \textcolor{gray}{// hier hat PE \( i \) Element \( x_i \)} \\
  \( \text{theoQSort}(x,0,p-1) \) \\
  \ \\
  \textcolor{gray}{// Teil 1: kleine Elemente zählen} \\
  \textbf{\textsc{theoQSort}}\( (x,\dots) \) \\
  \textbf{if} \( i \equiv 0 \) \textbf{then} \( \text{ipv} \coloneqq \text{randInt}(0,p) \) \\
  \( \text{ipv} \coloneqq \text{bcast}(\text{ipv}, 0) \) \\
  \( \text{pv} \coloneqq \text{bcast}(x, \text{ipv}) \) \\
  \( \text{small} \coloneqq (x \leq \text{pv}) \) \\
  \( j \coloneqq \text{prefixSum}(\text{small},+) \) \\
  \( p' \coloneqq \text{bcast}(j,p-1) \) \\
  \ \\
  \textcolor{gray}{// Teil 2: Datenumverteilung und Rekursion} \\
  \textbf{if} \( \text{small} \equiv 1 \) \textbf{then} \( \text{send}(x,j-1) \) \\
  \textbf{else} \( \text{send}(x,p'+i-j) \) \\
  \( x \coloneqq \text{recv}(\text{any}) \) \\
  recursive theoQSort of ``left''/``right'' part
\end{pseudocode}

Die erwartete Rekursionstiefe ist in \( O(\log p) \), die Zeit jeweils in \( O(T_\text{start}\log p) \). Insgesamt ist die erwartete Zeit also in \( O(T_\text{start}(\log p)^2) \).